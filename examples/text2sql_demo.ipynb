{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL Agent - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the key features of the Text-to-SQL Agent system:\n",
    "\n",
    "1. **Schema Management** - Loading database schema from Excel\n",
    "2. **Semantic Join Inference** - Finding joins without explicit foreign keys\n",
    "3. **Session Management** - Tracking and persisting agent state\n",
    "4. **Correction System** - Learning from user feedback\n",
    "5. **BigQuery Integration** - Executing and validating queries\n",
    "6. **Error Recovery** - Handling API failures gracefully\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "1. Install dependencies: `pip install -r ../requirements.txt`\n",
    "2. Configure `.env` file with your credentials\n",
    "3. Prepare your schema Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import main components\n",
    "from src import (\n",
    "    settings,\n",
    "    schema_loader,\n",
    "    bigquery_client,\n",
    "    azure_client,\n",
    "    session_manager,\n",
    "    AgentState,\n",
    "    JoinInference,\n",
    "    CorrectionParser,\n",
    ")\n",
    "\n",
    "from src.utils import (\n",
    "    RetryExhaustedError,\n",
    "    FatalError,\n",
    "    AmbiguityError,\n",
    "    setup_logger,\n",
    ")\n",
    "\n",
    "# For nice display\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Check\n",
    "\n",
    "Let's verify that the system is properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check configuration\n",
    "print(\"Configuration Status:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    azure_endpoint = settings.get(\"azure_openai.endpoint\")\n",
    "    print(f\"‚úÖ Azure OpenAI Endpoint: {azure_endpoint[:50]}...\")\n",
    "except:\n",
    "    print(\"‚ùå Azure OpenAI not configured\")\n",
    "\n",
    "try:\n",
    "    project_id = settings.get(\"bigquery.project_id\")\n",
    "    dataset = settings.get(\"bigquery.dataset\")\n",
    "    print(f\"‚úÖ BigQuery Project: {project_id}\")\n",
    "    print(f\"‚úÖ BigQuery Dataset: {dataset}\")\n",
    "except:\n",
    "    print(\"‚ùå BigQuery not configured\")\n",
    "\n",
    "try:\n",
    "    schema_path = settings.get(\"schema.excel_path\")\n",
    "    print(f\"‚úÖ Schema Path: {schema_path}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Schema path not set (will need to provide manually)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Loading\n",
    "\n",
    "Load database schema from Excel file containing table and column metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from configured path\n",
    "try:\n",
    "    schema = schema_loader.load_from_excel()\n",
    "    print(f\"‚úÖ Loaded schema from configured path\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load from configured path: {e}\")\n",
    "    print(\"\\nOption 2: Provide path manually:\")\n",
    "    print(\"schema = schema_loader.load_from_excel(excel_path='/path/to/your/schema.xlsx')\")\n",
    "    \n",
    "    # For demo purposes, create a mock schema\n",
    "    from src.schema import Schema, Table, Column, ColumnType\n",
    "    \n",
    "    schema = Schema(project_id=\"demo-project\", dataset=\"demo_dataset\")\n",
    "    \n",
    "    # Create Customers table\n",
    "    customers = Table(name=\"Customers\", description=\"Customer master data\")\n",
    "    customers.add_column(Column(name=\"customer_id\", data_type=ColumnType.INTEGER, is_primary=True))\n",
    "    customers.add_column(Column(name=\"customer_name\", data_type=ColumnType.STRING))\n",
    "    customers.add_column(Column(name=\"region\", data_type=ColumnType.STRING, description=\"Geographic region\"))\n",
    "    customers.add_column(Column(name=\"account_status\", data_type=ColumnType.STRING))\n",
    "    schema.add_table(customers)\n",
    "    \n",
    "    # Create Orders table\n",
    "    orders = Table(name=\"Orders\", description=\"Customer orders\")\n",
    "    orders.add_column(Column(name=\"order_id\", data_type=ColumnType.INTEGER, is_primary=True))\n",
    "    orders.add_column(Column(name=\"customer_id\", data_type=ColumnType.INTEGER, description=\"Reference to customer\"))\n",
    "    orders.add_column(Column(name=\"order_date\", data_type=ColumnType.DATE))\n",
    "    orders.add_column(Column(name=\"amount\", data_type=ColumnType.FLOAT))\n",
    "    schema.add_table(orders)\n",
    "    \n",
    "    # Create Products table\n",
    "    products = Table(name=\"Products\", description=\"Product catalog\")\n",
    "    products.add_column(Column(name=\"product_id\", data_type=ColumnType.INTEGER, is_primary=True))\n",
    "    products.add_column(Column(name=\"product_name\", data_type=ColumnType.STRING))\n",
    "    products.add_column(Column(name=\"category\", data_type=ColumnType.STRING))\n",
    "    products.add_column(Column(name=\"price\", data_type=ColumnType.FLOAT))\n",
    "    schema.add_table(products)\n",
    "    \n",
    "    print(\"\\nüìù Created demo schema for illustration purposes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema summary\n",
    "print(f\"\\nüìä Schema Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project: {schema.project_id}\")\n",
    "print(f\"Dataset: {schema.dataset}\")\n",
    "print(f\"Total Tables: {len(schema.tables)}\")\n",
    "print(\"\\nTables:\")\n",
    "\n",
    "for table_name, table in schema.tables.items():\n",
    "    print(f\"\\n  üìã {table_name}\")\n",
    "    if table.description:\n",
    "        print(f\"     {table.description}\")\n",
    "    print(f\"     Columns: {len(table.columns)}\")\n",
    "    \n",
    "    # Show first few columns\n",
    "    for col in table.columns[:5]:\n",
    "        indicators = []\n",
    "        if col.is_primary:\n",
    "            indicators.append(\"üîë PK\")\n",
    "        if col.is_pii:\n",
    "            indicators.append(\"üîí PII\")\n",
    "        indicator_str = \" \".join(indicators)\n",
    "        print(f\"       ‚Ä¢ {col.name} ({col.data_type.value}) {indicator_str}\")\n",
    "    \n",
    "    if len(table.columns) > 5:\n",
    "        print(f\"       ... and {len(table.columns) - 5} more columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Join Inference\n",
    "\n",
    "The system can automatically infer how to join tables even without explicit foreign keys, using:\n",
    "- Column name similarity\n",
    "- Business name matching\n",
    "- Data type compatibility\n",
    "- LLM semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize join inference\n",
    "join_inference = JoinInference(schema, confidence_threshold=0.70)\n",
    "\n",
    "# Get table names\n",
    "table_names = list(schema.tables.keys())\n",
    "print(f\"Available tables: {table_names}\")\n",
    "\n",
    "if len(table_names) >= 2:\n",
    "    table1, table2 = table_names[0], table_names[1]\n",
    "    \n",
    "    print(f\"\\nüîç Inferring joins between: {table1} ‚Üî {table2}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        joins = join_inference.infer_joins(table1, table2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(joins)} possible join(s):\\n\")\n",
    "        \n",
    "        for i, join in enumerate(joins, 1):\n",
    "            confidence_emoji = \"üü¢\" if join.confidence >= 0.9 else \"üü°\" if join.confidence >= 0.7 else \"üü†\"\n",
    "            print(f\"{confidence_emoji} Option {i}:\")\n",
    "            print(f\"   SQL: {join.to_sql_condition()}\")\n",
    "            print(f\"   Confidence: {join.confidence:.1%}\")\n",
    "            print(f\"   Reasoning: {join.reasoning}\")\n",
    "            print()\n",
    "            \n",
    "    except AmbiguityError as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Ambiguity Detected!\")\n",
    "        print(f\"\\nMessage: {e}\")\n",
    "        print(f\"\\nOptions to choose from:\")\n",
    "        for i, opt in enumerate(e.options, 1):\n",
    "            print(f\"  {i}. {opt}\")\n",
    "        print(\"\\nüí° User would be prompted to select the correct option\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Need at least 2 tables for join inference demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Join Inference\n",
    "\n",
    "You can also manually specify which tables to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Infer joins between specific tables\n",
    "if \"Customers\" in schema.tables and \"Orders\" in schema.tables:\n",
    "    print(\"üîç Analyzing: Customers ‚Üî Orders\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        joins = join_inference.infer_joins(\"Customers\", \"Orders\")\n",
    "        \n",
    "        if joins:\n",
    "            best_join = joins[0]\n",
    "            print(f\"\\n‚ú® Best join found:\")\n",
    "            print(f\"   {best_join.to_sql_condition()}\")\n",
    "            print(f\"   Confidence: {best_join.confidence:.1%}\")\n",
    "            \n",
    "            # Show as SQL\n",
    "            sql_example = f\"\"\"\n",
    "SELECT c.customer_name, COUNT(o.order_id) as total_orders\n",
    "FROM Customers c\n",
    "JOIN Orders o ON {best_join.to_sql_condition()}\n",
    "GROUP BY c.customer_name\n",
    "\"\"\"\n",
    "            print(f\"\\nüìù Example SQL usage:\")\n",
    "            print(sql_example)\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Customers and Orders tables not found in schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Session Management\n",
    "\n",
    "Sessions track the entire conversation and can be saved/resumed at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new session\n",
    "user_query = \"Show me the top 5 customers by total order amount in Q4 2025\"\n",
    "\n",
    "print(f\"üí¨ User Query: '{user_query}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "session = session_manager.create_session(user_query)\n",
    "\n",
    "print(f\"\\n‚úÖ Session created\")\n",
    "print(f\"   Session ID: {session.session_id}\")\n",
    "print(f\"   Status: {session.status}\")\n",
    "print(f\"   Created: {session.created_at.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agent workflow\n",
    "print(\"ü§ñ Agent Workflow Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Add user message\n",
    "session.add_message(\"user\", user_query)\n",
    "print(\"1Ô∏è‚É£  Added user message\")\n",
    "\n",
    "# Step 2: Transition to query understanding\n",
    "session.state_machine.transition_to(\n",
    "    AgentState.QUERY_UNDERSTANDING,\n",
    "    reason=\"Starting query analysis\"\n",
    ")\n",
    "print(f\"2Ô∏è‚É£  State: {session.state_machine.current_state.value}\")\n",
    "\n",
    "# Step 3: Identify tables\n",
    "session.identified_tables = [\"Customers\", \"Orders\"]\n",
    "session.add_intermediate_result(\n",
    "    \"identified_tables\",\n",
    "    {\"tables\": session.identified_tables, \"confidence\": 0.95}\n",
    ")\n",
    "print(f\"3Ô∏è‚É£  Identified tables: {session.identified_tables}\")\n",
    "\n",
    "# Step 4: Join inference\n",
    "session.state_machine.transition_to(\n",
    "    AgentState.JOIN_INFERENCE,\n",
    "    reason=\"Inferring table joins\"\n",
    ")\n",
    "\n",
    "if len(session.identified_tables) >= 2:\n",
    "    try:\n",
    "        joins = join_inference.infer_joins(\n",
    "            session.identified_tables[0],\n",
    "            session.identified_tables[1]\n",
    "        )\n",
    "        session.inferred_joins = [j.to_dict() for j in joins]\n",
    "        print(f\"4Ô∏è‚É£  Inferred {len(joins)} join(s)\")\n",
    "    except:\n",
    "        print(\"4Ô∏è‚É£  Join inference skipped (demo mode)\")\n",
    "\n",
    "# Step 5: Increment iteration\n",
    "session.increment_iteration()\n",
    "print(f\"5Ô∏è‚É£  Iteration: {session.iteration_count}\")\n",
    "\n",
    "# Step 6: Save session\n",
    "session_manager.save_session(session)\n",
    "print(f\"6Ô∏è‚É£  Session saved to disk\")\n",
    "\n",
    "print(f\"\\n‚úÖ Workflow complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View session details\n",
    "print(\"üìä Session Details\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Session ID: {session.session_id}\")\n",
    "print(f\"Status: {session.status}\")\n",
    "print(f\"Current State: {session.state_machine.current_state.value}\")\n",
    "print(f\"Iterations: {session.iteration_count}\")\n",
    "print(f\"Messages: {len(session.messages)}\")\n",
    "print(f\"Identified Tables: {session.identified_tables}\")\n",
    "print(f\"Inferred Joins: {len(session.inferred_joins)}\")\n",
    "print(f\"\\nState Transitions:\")\n",
    "for i, trans in enumerate(session.state_machine.get_transition_history(), 1):\n",
    "    print(f\"  {i}. {trans['from_state']} ‚Üí {trans['to_state']}\")\n",
    "    if trans['reason']:\n",
    "        print(f\"     Reason: {trans['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List All Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List recent sessions\n",
    "sessions = session_manager.list_sessions(limit=10)\n",
    "\n",
    "print(f\"üìã Recent Sessions ({len(sessions)} found)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if sessions:\n",
    "    # Create DataFrame for nice display\n",
    "    df = pd.DataFrame(sessions)\n",
    "    df['session_id'] = df['session_id'].str[:8] + '...'  # Truncate for display\n",
    "    df['query'] = df['query'].str[:50] + '...'  # Truncate long queries\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No sessions found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume the session we just created\n",
    "print(f\"üîÑ Resuming session: {session.session_id[:8]}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resumed_session = session_manager.load_session(session.session_id)\n",
    "\n",
    "print(f\"‚úÖ Session resumed successfully!\")\n",
    "print(f\"   Query: {resumed_session.original_query}\")\n",
    "print(f\"   State: {resumed_session.state_machine.current_state.value}\")\n",
    "print(f\"   Iteration: {resumed_session.iteration_count}\")\n",
    "print(f\"   Messages: {len(resumed_session.messages)}\")\n",
    "print(f\"\\nüí° The session can now continue from where it left off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correction System\n",
    "\n",
    "Users can provide corrections to guide the agent when it makes mistakes or encounters ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß User Correction Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a session for corrections demo\n",
    "correction_session = session_manager.create_session(\"Demo for corrections\")\n",
    "\n",
    "# Example 1: Join clarification\n",
    "print(\"\\n1Ô∏è‚É£  Join Clarification\")\n",
    "correction1 = CorrectionParser.parse(\"join Customers.customer_id with Orders.customer_id\")\n",
    "print(f\"   Input: 'join Customers.customer_id with Orders.customer_id'\")\n",
    "print(f\"   Type: {correction1.correction_type.value}\")\n",
    "print(f\"   Content: {correction1.content}\")\n",
    "print(f\"   Constraint: {correction1.to_constraint_string()}\")\n",
    "\n",
    "correction_session.add_correction(correction1)\n",
    "\n",
    "# Example 2: Column mapping\n",
    "print(\"\\n2Ô∏è‚É£  Column Mapping\")\n",
    "correction2 = CorrectionParser.parse(\"region means Customers.geographic_area\")\n",
    "print(f\"   Input: 'region means Customers.geographic_area'\")\n",
    "print(f\"   Type: {correction2.correction_type.value}\")\n",
    "print(f\"   Content: {correction2.content}\")\n",
    "print(f\"   Constraint: {correction2.to_constraint_string()}\")\n",
    "\n",
    "correction_session.add_correction(correction2)\n",
    "\n",
    "# Example 3: Natural language correction\n",
    "print(\"\\n3Ô∏è‚É£  Natural Language Correction\")\n",
    "correction3 = CorrectionParser.parse(\n",
    "    \"Use the customer_id field from Orders table, not the account_number field\"\n",
    ")\n",
    "print(f\"   Input: 'Use the customer_id field from Orders table...'\")\n",
    "print(f\"   Type: {correction3.correction_type.value}\")\n",
    "print(f\"   Content: {correction3.content}\")\n",
    "print(f\"   Constraint: {correction3.to_constraint_string()}\")\n",
    "\n",
    "correction_session.add_correction(correction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all corrections in session\n",
    "print(\"\\nüìù Session Corrections Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total corrections: {len(correction_session.corrections)}\")\n",
    "print(f\"\\nHard constraints (applied to LLM prompts):\")\n",
    "for i, constraint in enumerate(correction_session.hard_constraints, 1):\n",
    "    print(f\"  {i}. {constraint}\")\n",
    "\n",
    "print(\"\\nüí° These constraints will be included in all future LLM prompts\")\n",
    "print(\"   to ensure the agent follows user's specifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Correction Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrections can also be provided in structured format\n",
    "print(\"üéØ Structured Correction Format\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "structured_correction = {\n",
    "    \"type\": \"join\",\n",
    "    \"tables\": [\"Orders\", \"Products\"],\n",
    "    \"join_condition\": \"Orders.product_id = Products.product_id\",\n",
    "    \"description\": \"Correct join for order-product relationship\"\n",
    "}\n",
    "\n",
    "correction = CorrectionParser.parse_dict(structured_correction)\n",
    "\n",
    "print(f\"Input (JSON):\")\n",
    "print(json.dumps(structured_correction, indent=2))\n",
    "print(f\"\\nParsed correction:\")\n",
    "print(f\"  Type: {correction.correction_type.value}\")\n",
    "print(f\"  Constraint: {correction.to_constraint_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BigQuery Integration\n",
    "\n",
    "Execute and validate SQL queries against BigQuery.\n",
    "\n",
    "**Note**: These operations require valid BigQuery credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query (modify for your schema)\n",
    "test_query = f\"\"\"\n",
    "SELECT\n",
    "    table_name,\n",
    "    row_count\n",
    "FROM `{settings.get('bigquery.project_id')}.{settings.get('bigquery.dataset')}.__TABLES__`\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è  BigQuery Operations Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuery:\")\n",
    "print(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Validate Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"1Ô∏è‚É£  Validating query...\")\n",
    "    validation = bigquery_client.validate_query(test_query)\n",
    "    \n",
    "    if validation[\"success\"]:\n",
    "        print(f\"   ‚úÖ Query is valid\")\n",
    "        print(f\"   üìä Bytes to process: {validation.get('bytes_processed', 0):,}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Validation failed: {validation.get('error')}\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Validation skipped: {e}\")\n",
    "    print(\"   (Make sure BigQuery credentials are configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Estimate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"2Ô∏è‚É£  Estimating query cost...\")\n",
    "    cost_info = bigquery_client.estimate_query_cost(test_query)\n",
    "    \n",
    "    if cost_info[\"success\"]:\n",
    "        print(f\"   üí∞ Estimated cost: ${cost_info['estimated_cost_usd']:.6f}\")\n",
    "        print(f\"   üì¶ Data size: {cost_info['readable_size']}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Cost estimation failed: {cost_info.get('error')}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Cost estimation skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Execute Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"3Ô∏è‚É£  Executing query...\")\n",
    "    result = bigquery_client.execute_query(test_query, max_results=10)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"   ‚úÖ Query successful!\")\n",
    "        print(f\"   üìä Rows returned: {result['row_count']}\")\n",
    "        print(f\"   üì¶ Bytes processed: {result['bytes_processed']:,}\")\n",
    "        \n",
    "        # Display results as DataFrame\n",
    "        if result['rows']:\n",
    "            print(f\"\\n   Results:\")\n",
    "            df_results = pd.DataFrame(result['rows'])\n",
    "            display(df_results)\n",
    "    else:\n",
    "        print(f\"   ‚ùå Query failed: {result['error']}\")\n",
    "        print(f\"   Error type: {result.get('error_type')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Execution skipped: {e}\")\n",
    "    print(f\"\\n   üí° To enable BigQuery operations:\")\n",
    "    print(f\"      1. Set up Google Cloud credentials\")\n",
    "    print(f\"      2. Configure GOOGLE_APPLICATION_CREDENTIALS in .env\")\n",
    "    print(f\"      3. Set GCP_PROJECT_ID and BIGQUERY_DATASET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Azure OpenAI Integration with Retry\n",
    "\n",
    "Make LLM calls with automatic retry on failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Azure OpenAI Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a session for this demo\n",
    "llm_session = session_manager.create_session(\"Test LLM capabilities\")\n",
    "\n",
    "test_prompt = \"Explain what a database foreign key is in one sentence.\"\n",
    "\n",
    "print(f\"\\nüìù Prompt: '{test_prompt}'\")\n",
    "print(f\"\\nüîÑ Making API call with automatic retry...\\n\")\n",
    "\n",
    "try:\n",
    "    response = azure_client.chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful database assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": test_prompt\n",
    "            }\n",
    "        ],\n",
    "        session=llm_session,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Response received:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüìä Session updated:\")\n",
    "    print(f\"   Messages: {len(llm_session.messages)}\")\n",
    "    \n",
    "except RetryExhaustedError as e:\n",
    "    print(f\"‚ùå All retry attempts failed: {e}\")\n",
    "    print(f\"\\nüíæ Session {llm_session.session_id[:8]}... has been saved\")\n",
    "    print(f\"   You can resume it later when the service is available\")\n",
    "    print(f\"\\n   Command: session_manager.load_session('{llm_session.session_id}')\")\n",
    "    \n",
    "except FatalError as e:\n",
    "    print(f\"‚ùå Non-recoverable error: {e}\")\n",
    "    print(f\"\\nüí° Check your Azure OpenAI configuration:\")\n",
    "    print(f\"   - AZURE_OPENAI_ENDPOINT\")\n",
    "    print(f\"   - AZURE_OPENAI_API_KEY\")\n",
    "    print(f\"   - AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not make LLM call: {e}\")\n",
    "    print(f\"\\nüí° This is expected if Azure OpenAI is not configured.\")\n",
    "    print(f\"   The retry mechanism would handle temporary failures automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End Example\n",
    "\n",
    "Putting it all together: Complete workflow from query to SQL generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ End-to-End Workflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# User query\n",
    "user_query = \"What are the top 5 customers by total spending?\"\n",
    "print(f\"\\nüë§ User Query: '{user_query}'\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Step 1: Create session\n",
    "print(\"\\n1Ô∏è‚É£  Creating session...\")\n",
    "workflow_session = session_manager.create_session(user_query)\n",
    "workflow_session.add_message(\"user\", user_query)\n",
    "print(f\"   ‚úÖ Session ID: {workflow_session.session_id[:8]}...\")\n",
    "\n",
    "# Step 2: Load schema\n",
    "print(\"\\n2Ô∏è‚É£  Loading schema...\")\n",
    "workflow_session.schema_snapshot = schema.to_dict()\n",
    "workflow_session.state_machine.transition_to(AgentState.SCHEMA_LOADING)\n",
    "print(f\"   ‚úÖ Loaded {len(schema.tables)} tables\")\n",
    "\n",
    "# Step 3: Identify relevant tables\n",
    "print(\"\\n3Ô∏è‚É£  Identifying relevant tables...\")\n",
    "workflow_session.state_machine.transition_to(AgentState.QUERY_UNDERSTANDING)\n",
    "workflow_session.identified_tables = [\"Customers\", \"Orders\"]  # Would be done by LLM\n",
    "print(f\"   ‚úÖ Identified: {workflow_session.identified_tables}\")\n",
    "\n",
    "# Step 4: Infer joins\n",
    "print(\"\\n4Ô∏è‚É£  Inferring table joins...\")\n",
    "workflow_session.state_machine.transition_to(AgentState.JOIN_INFERENCE)\n",
    "try:\n",
    "    if len(workflow_session.identified_tables) >= 2:\n",
    "        joins = join_inference.infer_joins(\n",
    "            workflow_session.identified_tables[0],\n",
    "            workflow_session.identified_tables[1]\n",
    "        )\n",
    "        workflow_session.inferred_joins = [j.to_dict() for j in joins]\n",
    "        print(f\"   ‚úÖ Found {len(joins)} join(s)\")\n",
    "        if joins:\n",
    "            print(f\"   üìä Best: {joins[0].to_sql_condition()} (confidence: {joins[0].confidence:.1%})\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Using demo joins: {e}\")\n",
    "    workflow_session.inferred_joins = [{\n",
    "        \"left_table\": \"Customers\",\n",
    "        \"right_table\": \"Orders\",\n",
    "        \"left_column\": \"customer_id\",\n",
    "        \"right_column\": \"customer_id\",\n",
    "        \"confidence\": 0.95\n",
    "    }]\n",
    "\n",
    "# Step 5: Generate SQL (simulated)\n",
    "print(\"\\n5Ô∏è‚É£  Generating SQL query...\")\n",
    "workflow_session.state_machine.transition_to(AgentState.GENERATING_SQL)\n",
    "\n",
    "# Simulated SQL generation (would normally use LLM)\n",
    "if workflow_session.inferred_joins:\n",
    "    join_info = workflow_session.inferred_joins[0]\n",
    "    generated_sql = f\"\"\"\n",
    "SELECT\n",
    "    c.customer_name,\n",
    "    SUM(o.amount) as total_spending\n",
    "FROM {schema.dataset}.Customers c\n",
    "JOIN {schema.dataset}.Orders o\n",
    "    ON c.{join_info['left_column']} = o.{join_info['right_column']}\n",
    "GROUP BY c.customer_name\n",
    "ORDER BY total_spending DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "else:\n",
    "    generated_sql = \"-- SQL generation would happen here\"\n",
    "\n",
    "workflow_session.add_sql_attempt(generated_sql, success=True)\n",
    "workflow_session.increment_iteration()\n",
    "\n",
    "print(f\"   ‚úÖ SQL generated\")\n",
    "\n",
    "# Step 6: Complete workflow\n",
    "print(\"\\n6Ô∏è‚É£  Finalizing...\")\n",
    "workflow_session.state_machine.transition_to(AgentState.COMPLETED)\n",
    "session_manager.save_session(workflow_session)\n",
    "print(f\"   ‚úÖ Session saved with status: {workflow_session.status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® Workflow Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated SQL\n",
    "print(\"\\nüìù Generated SQL Query:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_sql)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display workflow summary\n",
    "print(\"\\nüìä Workflow Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = {\n",
    "    \"Session ID\": workflow_session.session_id,\n",
    "    \"Original Query\": workflow_session.original_query,\n",
    "    \"Final State\": workflow_session.state_machine.current_state.value,\n",
    "    \"Status\": workflow_session.status,\n",
    "    \"Iterations\": workflow_session.iteration_count,\n",
    "    \"Tables Identified\": len(workflow_session.identified_tables),\n",
    "    \"Joins Inferred\": len(workflow_session.inferred_joins),\n",
    "    \"SQL Attempts\": len(workflow_session.sql_attempts),\n",
    "    \"Corrections Applied\": len(workflow_session.corrections),\n",
    "}\n",
    "\n",
    "for key, value in summary_data.items():\n",
    "    print(f\"  {key:.<40} {value}\")\n",
    "\n",
    "print(\"\\nüìà State Transition History:\")\n",
    "for i, trans in enumerate(workflow_session.state_machine.get_transition_history(), 1):\n",
    "    print(f\"  {i}. {trans['from_state']:.<25} ‚Üí {trans['to_state']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key capabilities of the Text-to-SQL Agent:\n",
    "\n",
    "‚úÖ **Schema Management** - Load and explore database metadata from Excel  \n",
    "‚úÖ **Semantic Join Inference** - Automatically find table relationships  \n",
    "‚úÖ **Session Persistence** - Save and resume agent state  \n",
    "‚úÖ **User Corrections** - Learn from feedback and improve  \n",
    "‚úÖ **BigQuery Integration** - Validate and execute queries  \n",
    "‚úÖ **Error Recovery** - Handle API failures gracefully  \n",
    "‚úÖ **End-to-End Workflow** - Complete query processing pipeline  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Configure your environment** with real credentials\n",
    "2. **Prepare your schema** Excel file\n",
    "3. **Try real queries** against your BigQuery dataset\n",
    "4. **Extend the system** with custom reasoning modules\n",
    "5. **Build a UI** on top of this framework\n",
    "\n",
    "---\n",
    "\n",
    "**For more information, see:**\n",
    "- [README.md](../README.md) - Setup and usage guide\n",
    "- [ARCHITECTURE.md](../ARCHITECTURE.md) - System design documentation\n",
    "- [example_usage.py](example_usage.py) - Python examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
